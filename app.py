# app.py
# -*- coding: utf-8 -*-
"""
æç®€äº”è¨€ç»å¥ç”Ÿæˆï¼ˆæ¨ç†ç‰ˆï¼Œå«å‰ç½®æç¤ºè¯ï¼‰
- å¿…å¡«ï¼šå‰ç½®æç¤ºè¯ï¼ˆè‡³å°‘ 1 ä¸ªå­—ï¼‰
- å›ºå®šæ€»å­—æ•°ï¼š24ï¼ˆåŒ…å«å‰ç½®æç¤ºè¯ï¼‰
- åªå±•ç¤ºæ ¼å¼åŒ–ç»“æœï¼š4 è¡Œ Ã— 6 å­—
- ä¾èµ–æ–‡ä»¶ï¼špoetry_vocabulary.txt, lstm_poetry_model.keras
"""
import os
import numpy as np
import gradio as gr

from keras import models, layers

VOCAB_PATH = os.environ.get("POETRY_VOCAB_PATH", "models/poetry_vocabulary.txt")
MODEL_PATH = os.environ.get("POETRY_MODEL_PATH", "models/lstm_poetry_model-epoch50.keras")
TOTAL_CHARS = 24  # æœ€ç»ˆæ€»å­—æ•°ï¼ŒåŒ…å«å‰ç½®æç¤ºè¯

# -------- è¯è¡¨åŠ è½½ --------
def load_text_vectorization(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¯è¡¨ï¼š{path}")
    with open(path, "r", encoding="utf-8") as f:
        vocab = [line.rstrip("\n") for line in f]
    if not vocab:
        raise ValueError("è¯è¡¨ä¸ºç©º")

    tv = layers.TextVectorization(
        standardize=None,
        split='character',
        output_mode="int",
        output_sequence_length=None,
        ragged = True
    )
    tv.set_vocabulary(vocab)
    return tv

text_vectorization = load_text_vectorization(VOCAB_PATH)
VOCAB_SIZE = len(text_vectorization.get_vocabulary())

# -------- æ¨¡å‹åŠ è½½ --------
model = models.load_model(MODEL_PATH, compile=False)
out_dim = model.output_shape[-1]
if out_dim != VOCAB_SIZE:
    raise ValueError(f"æ¨¡å‹è¾“å‡ºç»´åº¦({out_dim})ä¸è¯è¡¨å¤§å°({VOCAB_SIZE})ä¸ä¸€è‡´")

# Copy the generate and sampling functions from lstm_generative_model.ipynb
def generate(prompt: str, max_length: int, temperature = 1.0) -> str:
    """
    Generate a poem based on the start prompt

    Returns:
        A generated poem as a string.
    """
    generated = list(text_vectorization(prompt)[:len(prompt)])
    while len(generated) < max_length:
        input_sequence = np.array(generated).reshape(1, -1)
        predictions = model.predict(input_sequence, verbose=0)[0]
        next_token_id = sampling(predictions[-1], temperature)
        generated.append(next_token_id)
    return ''.join(text_vectorization.get_vocabulary()[token_id] for token_id in generated)

def sampling(predictions: list, temperature=1.0, eps1=1e-20, eps2=1e-9) -> int:
    p = np.asarray(predictions, dtype=np.float64)

    # The two key points: log(p + eps1) divide by (T + eps2)
    logits = np.log(p + eps1) / (float(temperature) + eps2)

    # Subtract the max logit to prevent overflow
    logits -= np.max(logits)

    q = np.exp(logits)
    q /= q.sum()
    return int(np.random.choice(len(q), p=q))

# -------- ç”Ÿæˆä¸æ ¼å¼åŒ– --------
def generate_and_format(prompt, temperature=1.0):
    # ç”Ÿæˆæ­£å¸¸çš„å¤è¯—
    poem = generate(prompt, 24, temperature=temperature)

    # æ ¼å¼åŒ–ä¸º 4Ã—6
    lines = ["".join(poem[i:i+6]) for i in range(0, TOTAL_CHARS, 6)]
    return "\n".join(lines)

# -------- Gradio UI --------
DESC = "# äº”è¨€ç»å¥ç”Ÿæˆ\nè¯·è¾“å…¥**è‡³å°‘ 1 ä¸ªèµ·å§‹å­—**ï¼Œæ€»å­—æ•°å›ºå®š 24ï¼ˆåŒ…å«èµ·å§‹å­—ï¼‰ï¼Œä»…å±•ç¤º 4Ã—6 æ’ç‰ˆç»“æœã€‚"

with gr.Blocks(title="äº”è¨€ç»å¥ç”Ÿæˆ") as demo:
    gr.Markdown(DESC)
    with gr.Row():
        seed = gr.Textbox(label="å‰ç½®æç¤ºè¯ï¼ˆè‡³å°‘ 1 ä¸ªå­—ï¼‰", placeholder="ä¾‹å¦‚ï¼šæµ·å¤–", lines=1)
    with gr.Row():
        temp = gr.Slider(0, 2.0, value=0.5, step=0.05, label="æ¸©åº¦ï¼ˆ0=è´ªå¿ƒï¼‰")
    btn = gr.Button("ğŸ“ ç”Ÿæˆ")
    output = gr.Textbox(label="ç”Ÿæˆç»“æœ", lines=6)

    btn.click(fn=generate_and_format, inputs=[seed, temp], outputs=[output], api_name="generate")

    # é¡µè„šä¿¡æ¯
    with gr.Row():
        gr.Markdown(
            f"**æ¨¡å‹æ–‡ä»¶**ï¼š`{MODEL_PATH}` ï½œ **è¯è¡¨**ï¼š`{VOCAB_PATH}` ï½œ **è¯è¡¨å¤§å°**ï¼š{VOCAB_SIZE}"
        )

if __name__ == "__main__":
    port = int(os.environ.get("GRADIO_PORT", "7860"))
    demo.launch(server_name="0.0.0.0", server_port=port, share=False)
