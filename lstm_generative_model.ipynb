{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Use LSTM to generate the poem\n",
    "\n",
    "The procedure:\n",
    "\n",
    "1. Embedding Layer;\n",
    "2. LSTM Decoder-only;\n",
    "3. Sample for generation;\n",
    "4. Use all data to train, which overfits the training data."
   ],
   "id": "67d05df12625237f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:12:58.483335Z",
     "start_time": "2025-08-28T09:12:56.301768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%script echo skipping\n",
    "\n",
    "%pip install pandas"
   ],
   "id": "5cd8e1b35e1257b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Requirement already satisfied: pandas in /Users/run/anaconda3/envs/nlpia/lib/python3.12/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/run/anaconda3/envs/nlpia/lib/python3.12/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/run/anaconda3/envs/nlpia/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/run/anaconda3/envs/nlpia/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/run/anaconda3/envs/nlpia/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/run/anaconda3/envs/nlpia/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "VOCAB_PATH = os.environ.get(\"POETRY_VOCAB_PATH\", \"models/poetry_vocabulary.txt\")\n",
    "MODEL_PATH = os.environ.get(\"POETRY_MODEL_PATH\", \"models/lstm_poetry_model.keras\")"
   ],
   "id": "948bb3d95154d1a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Download The Dataset",
   "id": "243f99368fd6af89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:12:58.729353Z",
     "start_time": "2025-08-28T09:12:58.712225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Optional: set the proxy\n",
    "%env all_proxy=socks5://127.0.0.1:7897\n",
    "\n",
    "!mkdir -p data\n",
    "!git clone https://github.com/xiu-ze/Poetry.git data/Poetry"
   ],
   "id": "71814b0fc679536",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:12:58.754405Z",
     "start_time": "2025-08-28T09:12:58.743763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all files\n",
    "import os\n",
    "\n",
    "def get_all_files(base_dir):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "base_dir = os.path.expanduser('data/Poetry/诗歌数据集')\n",
    "poem_files = get_all_files(base_dir)\n",
    "poem_files"
   ],
   "id": "8488e291bbcafca6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Poetry/诗歌数据集/秦.csv',\n",
       " 'data/Poetry/诗歌数据集/先秦.csv',\n",
       " 'data/Poetry/诗歌数据集/隋.csv',\n",
       " 'data/Poetry/诗歌数据集/辽.csv',\n",
       " 'data/Poetry/诗歌数据集/当代.csv',\n",
       " 'data/Poetry/诗歌数据集/明_1.csv',\n",
       " 'data/Poetry/诗歌数据集/明_2.csv',\n",
       " 'data/Poetry/诗歌数据集/明_3.csv',\n",
       " 'data/Poetry/诗歌数据集/清_3.csv',\n",
       " 'data/Poetry/诗歌数据集/清_2.csv',\n",
       " 'data/Poetry/诗歌数据集/明_4.csv',\n",
       " 'data/Poetry/诗歌数据集/元.csv',\n",
       " 'data/Poetry/诗歌数据集/清_1.csv',\n",
       " 'data/Poetry/诗歌数据集/南北朝.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_1.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_2.csv',\n",
       " 'data/Poetry/诗歌数据集/唐.csv',\n",
       " 'data/Poetry/诗歌数据集/近现代.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_3.csv',\n",
       " 'data/Poetry/诗歌数据集/汉.csv',\n",
       " 'data/Poetry/诗歌数据集/金.csv',\n",
       " 'data/Poetry/诗歌数据集/魏晋.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Clean the dataset\n",
    "\n",
    "1. Truncate the poems to 24 characters;\n",
    "2. Check the invalid signs in the fixed positions and remove the abnormal items."
   ],
   "id": "67716901997dc298"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:12:59.527626Z",
     "start_time": "2025-08-28T09:12:58.777966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read dataset from one file\n",
    "import pandas as pd\n",
    "\n",
    "project_root = os.path.abspath('.')\n",
    "\n",
    "def read_file_to_df(file_path):\n",
    "    file = os.path.join(project_root, file_path)\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File {file} does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    filter_by_wujue = df[df[\"体裁\"].astype(str).str.contains(\"五言绝句\", na=False)].copy()\n",
    "    wujue_content = filter_by_wujue['内容']\n",
    "    return wujue_content"
   ],
   "id": "46885637a1419f00",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:12:59.547871Z",
     "start_time": "2025-08-28T09:12:59.539530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transform the pandas Series to numpy array and check\n",
    "import numpy as np\n",
    "\n",
    "def check_punctuation(poem_texts, positions=[5, 11, 17, 23]):\n",
    "    \"\"\"\n",
    "    Check the punctuation in the fixed positions of the poem texts.\n",
    "    It prints the found characters in the fixed positions and identifies any invalid characters.\n",
    "\n",
    "    :param poem_texts: np.ndarray, the array of poem texts\n",
    "    :return: set, invalid punctuations\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the fixed location values\n",
    "    chars_at_fixed_positions = set(poem_texts[:, positions].reshape(-1))\n",
    "    print('Characters found at fixed positions:', ''.join(chars_at_fixed_positions))\n",
    "\n",
    "    # Check the invalid characters\n",
    "    valid_chars = set(\"！，？。\")\n",
    "    invalid_chars = chars_at_fixed_positions - valid_chars\n",
    "    print('Find invalid characters:', ''.join(invalid_chars))\n",
    "\n",
    "    return invalid_chars\n",
    "\n",
    "def clean_poem_texts(poem_texts):\n",
    "    \"\"\"\n",
    "    Clean the poem texts by removing invalid characters and truncating to 24 characters.\n",
    "\n",
    "    :param poem_texts: pd.Series, the series of poem texts\n",
    "    :return: ndarray, cleaned poem texts with shape (n, 24)\n",
    "    \"\"\"\n",
    "    # Adjust the size of every poem item to 24\n",
    "    poems_truncated = poem_texts.map(lambda x: x[:24])\n",
    "\n",
    "    # Check the size less than 24\n",
    "    poems_small = poems_truncated[poems_truncated.str.len() < 24]\n",
    "    if not poems_small.empty:\n",
    "        print('Count of poems with size less than 24:', len(poems_small))\n",
    "        poems_truncated = poems_truncated[poems_truncated.str.len() == 24]\n",
    "\n",
    "    # Transform the pandas Series to numpy array\n",
    "    poem_numpy = np.array(\n",
    "        poems_truncated.map(list).to_list()\n",
    "    )\n",
    "    print('shape of numpy array', poem_numpy.shape)\n",
    "\n",
    "    # Check the signs in the wujue array\n",
    "    invalid_chars = check_punctuation(poem_numpy)\n",
    "\n",
    "    # Find the abnormal items\n",
    "    abnormal_items = poem_numpy[\n",
    "        np.isin(poem_numpy[:, [5, 11, 17, 23]], list(invalid_chars)).any(axis=1)\n",
    "    ]\n",
    "    abnormal_count = len(abnormal_items)\n",
    "    print('abnormal count:', abnormal_count)\n",
    "    if abnormal_count > 0:\n",
    "        print('abnormal item: ', ''.join(abnormal_items[0]))\n",
    "\n",
    "    # Remove the abnormal item\n",
    "    poems_removed_invalid = poem_numpy[\n",
    "        ~np.isin(poem_numpy[:, [5, 11, 17, 23]], list(invalid_chars)).any(axis=1)\n",
    "    ]\n",
    "\n",
    "    print('===== After removing the abnormal items =====')\n",
    "    print('shape of numpy array', poems_removed_invalid.shape)\n",
    "    check_punctuation(poems_removed_invalid)\n",
    "\n",
    "    # Convert back to pandas Series\n",
    "    return poems_removed_invalid"
   ],
   "id": "250d55c9d9594e57",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:05.533696Z",
     "start_time": "2025-08-28T09:12:59.560011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read all files and clean the dataset\n",
    "\n",
    "all_poems = []\n",
    "\n",
    "for file in poem_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    poem_texts = read_file_to_df(file)\n",
    "    if poem_texts.empty:\n",
    "        print(\"No valid poems found in this file.\\n\")\n",
    "        continue\n",
    "\n",
    "    result = clean_poem_texts(poem_texts)\n",
    "    all_poems.append(result)\n",
    "    print()\n",
    "\n",
    "# Concatenate all cleaned poems into a single array\n",
    "train_poems_numpy = np.concatenate(all_poems, axis=0)\n",
    "train_poems_numpy.shape"
   ],
   "id": "c85748c0068ea4ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data/Poetry/诗歌数据集/秦.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/先秦.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/隋.csv\n",
      "shape of numpy array (71, 24)\n",
      "Characters found at fixed positions: ，？。\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (71, 24)\n",
      "Characters found at fixed positions: ，？。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/辽.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/当代.csv\n",
      "shape of numpy array (1109, 24)\n",
      "Characters found at fixed positions: ，栏闲文：大轻」华？歌！冕帽；鼠宵。流对不边传山\n",
      "Find invalid characters: 传歌栏冕闲帽；文鼠：大宵轻」华流对不边山\n",
      "abnormal count: 13\n",
      "abnormal item:  琅玕经雨青，染绿山溪水。“萧萧我凭栏”，袅袅清歌\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1096, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_1.csv\n",
      "shape of numpy array (3712, 24)\n",
      "Characters found at fixed positions: 罗人歌相鹤自。月幕金丛者曲，过？兰》\n",
      "Find invalid characters: 罗人鹤相歌自月幕金丛者曲过兰》\n",
      "abnormal count: 5\n",
      "abnormal item:  片舫连双桨，呕哑杂哩罗。惯听吴棹曲，羞杀《阆中歌\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3707, 24)\n",
      "Characters found at fixed positions: ，？。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_2.csv\n",
      "Count of poems with size less than 24: 3\n",
      "shape of numpy array (3829, 24)\n",
      "Characters found at fixed positions: ，？。水\n",
      "Find invalid characters: 水\n",
      "abnormal count: 1\n",
      "abnormal item:  操杵力不任，当垆心自鄙。花时掩关坐，焚香读《秋水\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3828, 24)\n",
      "Characters found at fixed positions: ，？。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_3.csv\n",
      "shape of numpy array (4249, 24)\n",
      "Characters found at fixed positions: 传筝木。！；，？》学\n",
      "Find invalid characters: 筝木；传》学\n",
      "abnormal count: 6\n",
      "abnormal item:  雨馀鸟语凉，斜阳竹深见。频来非看花，借读《高僧传\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (4243, 24)\n",
      "Characters found at fixed positions: ，？。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_3.csv\n",
      "shape of numpy array (1882, 24)\n",
      "Characters found at fixed positions: 说，撞旧日1质天尽轻人3敲真话蛾弱？饶里尔亲乍！；多外断安磨销得思釭通伸。梅沾生遥近草诉\n",
      "Find invalid characters: 说撞旧日1质天尽轻人3敲真话蛾弱饶里尔亲乍；多外断安磨销得思釭通伸梅沾生遥近草诉\n",
      "abnormal count: 16\n",
      "abnormal item:  昔子贵之时，曾居洛阳市。洛阳人情何？曰『薄乎云尔\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1866, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_2.csv\n",
      "shape of numpy array (1473, 24)\n",
      "Characters found at fixed positions: 有。！；曲，？\n",
      "Find invalid characters: ；曲有\n",
      "abnormal count: 4\n",
      "abnormal item:  闻君向南投，曾过北投宿。试从鹿水头，一续「乌溪曲\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1469, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_4.csv\n",
      "shape of numpy array (2184, 24)\n",
      "Characters found at fixed positions: 。皇，？天\n",
      "Find invalid characters: 天皇\n",
      "abnormal count: 1\n",
      "abnormal item:  去去去此间，不是留侬处。侬住三十三天天外天，玉皇\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2183, 24)\n",
      "Characters found at fixed positions: ，？。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/元.csv\n",
      "shape of numpy array (2624, 24)\n",
      "Characters found at fixed positions: 南星：苧浮痕相篇？海歌！赋枝曲梅。两，经广扇\n",
      "Find invalid characters: 歌南经星扇赋枝曲：苧浮痕相梅两广篇海\n",
      "abnormal count: 9\n",
      "abnormal item:  老竹空岩里，悬厓飞水前。欲识逍遥境，试读《逍遥篇\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2615, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_1.csv\n",
      "shape of numpy array (1711, 24)\n",
      "Characters found at fixed positions: 箫人鹤水杜。！；城潮，？\n",
      "Find invalid characters: 箫人水鹤杜；城潮\n",
      "abnormal count: 4\n",
      "abnormal item:  昔作秦淮客，朱楼赋《洞箫》。白头故人尽，重上石城\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1707, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/南北朝.csv\n",
      "shape of numpy array (1, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_1.csv\n",
      "shape of numpy array (4397, 24)\n",
      "Characters found at fixed positions: 前莲。；，桨\n",
      "Find invalid characters: ；桨前莲\n",
      "abnormal count: 2\n",
      "abnormal item:  晚过鸳鸯浦，无心唱《采莲》。莫嗔兰桨急，为要趁前\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (4395, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_2.csv\n",
      "shape of numpy array (3470, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3470, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/唐.csv\n",
      "shape of numpy array (2711, 24)\n",
      "Characters found at fixed positions: 。此！，？沽\n",
      "Find invalid characters: 此沽\n",
      "abnormal count: 1\n",
      "abnormal item:  勒马问樵夫，前村酒有无。「杜康家在此，一任君来沽\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2710, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/近现代.csv\n",
      "shape of numpy array (398, 24)\n",
      "Characters found at fixed positions: 。！；，？\n",
      "Find invalid characters: ；\n",
      "abnormal count: 1\n",
      "abnormal item:  明月照秋霜，今朝还故乡；留得头颅在，雄心誓不降。\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (397, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_3.csv\n",
      "shape of numpy array (3005, 24)\n",
      "Characters found at fixed positions: 。花懒个曲灯，？\n",
      "Find invalid characters: 花个曲灯懒\n",
      "abnormal count: 3\n",
      "abnormal item:  明月入我池，皎皎铺纻缟。何日变成缁？《太玄》吾懒\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3002, 24)\n",
      "Characters found at fixed positions: ，？。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/汉.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/金.csv\n",
      "shape of numpy array (252, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (252, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/魏晋.csv\n",
      "No valid poems found in this file.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37012, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Dataset to token id sequences",
   "id": "44dc0adc2c6c2704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:15.961582Z",
     "start_time": "2025-08-28T09:13:05.550207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import layers\n",
    "\n",
    "tv = layers.TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    standardize=None,\n",
    "    split=None, # 直接喂二维数组\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=24\n",
    ")\n",
    "tv.adapt(train_poems_numpy)"
   ],
   "id": "c890cc6fbcb7117",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 17:13:09.192591: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:16.039554Z",
     "start_time": "2025-08-28T09:13:15.976385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demo usage of tv\n",
    "\n",
    "# Print the vocabulary\n",
    "print('Vocabulary size:', tv.vocabulary_size())\n",
    "print('Vocabulary:', ''.join(tv.get_vocabulary()[:20]))\n",
    "\n",
    "# Encode\n",
    "encoded = tv(train_poems_numpy[0])\n",
    "\n",
    "# Decode\n",
    "vocab = tv.get_vocabulary()\n",
    "decoded =[vocab[i] for i in encoded]\n",
    "print('Encoded:', encoded.numpy())\n",
    "print('Decoded:', ''.join(decoded))"
   ],
   "id": "ef849f42f169ce1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6350\n",
      "Vocabulary: [UNK]，。不人山风一花无来何云有日月春水中\n",
      "Encoded: [1152 1152  948  466   65    2 1074  177  740   41  604    3 1103   64\n",
      "    6   16 1230    2  945 2176  183    7   23    3]\n",
      "Decoded: 脉脉广川流，驱马历长洲。鹊飞山月曙，蝉噪野风秋。\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:16.234629Z",
     "start_time": "2025-08-28T09:13:16.055918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode all the poems\n",
    "train_token_ids = tv(train_poems_numpy)\n",
    "print('shape of wujue_token_ids:', train_token_ids.shape)"
   ],
   "id": "8277b7ae292b1f71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of wujue_token_ids: (37012, 24)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Build the LSTM Decoder model",
   "id": "6145e1fc59ad943a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:16.263078Z",
     "start_time": "2025-08-28T09:13:16.252014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the train dataset\n",
    "train_sequences = train_token_ids[:, :-1]\n",
    "target_sequences = train_token_ids[:, 1:]\n",
    "\n",
    "train_sequences.shape, target_sequences.shape"
   ],
   "id": "43b8b72f51002c3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([37012, 23]), TensorShape([37012, 23]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:16.558481Z",
     "start_time": "2025-08-28T09:13:16.292665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build a simple LSTM Decoder model\n",
    "\n",
    "import keras\n",
    "from keras import models, layers\n",
    "\n",
    "def build_model(vocab_size):\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"inputs\")\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=100, name=\"embedding\")(inputs)\n",
    "    x = layers.LSTM(512, return_sequences=True, name=\"lstm\")(x)\n",
    "    x = layers.Dropout(0.1, name=\"dropout\")(x)\n",
    "    outputs = layers.Dense(vocab_size, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs, name=\"lstm_decoder\")\n",
    "\n",
    "model = build_model(tv.vocabulary_size())\n",
    "model.summary()"
   ],
   "id": "75954408bd3f32c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"lstm_decoder\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"lstm_decoder\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (\u001B[38;5;33mInputLayer\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m)      │       \u001B[38;5;34m635,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │     \u001B[38;5;34m1,255,424\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001B[38;5;33mDense\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6350\u001B[0m)     │     \u001B[38;5;34m3,257,550\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">635,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,255,424</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6350</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,257,550</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m5,147,974\u001B[0m (19.64 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,147,974</span> (19.64 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m5,147,974\u001B[0m (19.64 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,147,974</span> (19.64 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:18.729510Z",
     "start_time": "2025-08-28T09:13:16.620690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the sample generate function\n",
    "def generate(prompt, max_length=24, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate a poem based on the start prompt\n",
    "\n",
    "    Returns:\n",
    "        A generated poem as a string.\n",
    "    \"\"\"\n",
    "    prompt_inputs = list(prompt)\n",
    "    generated = tv(prompt_inputs)[:len(prompt)].numpy().tolist()\n",
    "    while len(generated) < max_length:\n",
    "        input_sequence = np.array(generated).reshape(1, -1)\n",
    "        predictions = model.predict(input_sequence, verbose=0)[0]\n",
    "        next_token_id = sample(predictions[-1], temperature)\n",
    "        generated.append(next_token_id)\n",
    "    return ''.join(tv.get_vocabulary()[token_id] for token_id in generated)\n",
    "\n",
    "def sample(predictions, temperature=1.0, eps1=1e-20, eps2=1e-9):\n",
    "    p = np.asarray(predictions, dtype=np.float64)\n",
    "\n",
    "    # The two key points: log(p + eps1) divide by (T + eps2)\n",
    "    logits = np.log(p + eps1) / (float(temperature) + eps2)\n",
    "\n",
    "    # Subtract the max logit to prevent overflow\n",
    "    logits -= np.max(logits)\n",
    "\n",
    "    q = np.exp(logits)\n",
    "    q /= q.sum()\n",
    "    return int(np.random.choice(len(q), p=q))\n",
    "\n",
    "\n",
    "generate(\"海外\", temperature=0)"
   ],
   "id": "2145bf31d5cc14b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'海外粹拊揵修腪碑浼顸蔽荡巂蝼薯洵呼呼呼呼呼伧伧菶'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:18.754164Z",
     "start_time": "2025-08-28T09:13:18.746627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define callback to print the sample generative poem every 10 epochs\n",
    "class PoetryGenerateCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, epochs):\n",
    "        super().__init__()\n",
    "        self.generating_epochs = self._get_generating_epochs(epochs)\n",
    "        self.generated_poems = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch += 1\n",
    "        if epoch not in self.generating_epochs:\n",
    "            return\n",
    "        poems = self.generate_poems()\n",
    "        self.generated_poems[epoch] = {\n",
    "            'poems': poems,\n",
    "            'logs': logs\n",
    "        }\n",
    "\n",
    "        self.print_poems(poems)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_poems():\n",
    "        temperatures = [0, 0.5, 1.0, 1.5]\n",
    "        generated_texts = [\n",
    "            generate('海外', max_length=24, temperature=temp)\n",
    "            for temp in temperatures\n",
    "        ]\n",
    "        return [{ 'temperature': temperature, 'text': text } for temperature, text in zip(temperatures, generated_texts)]\n",
    "\n",
    "    @staticmethod\n",
    "    def print_poems(poems):\n",
    "        for item in poems:\n",
    "            print(f\"temperature {item['temperature']:.1f}: {item['text']}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_generating_epochs(epochs):\n",
    "        if epochs % 2 != 0:\n",
    "            print(\"Warning: epochs should be even number.\")\n",
    "\n",
    "        mid_epoch = epochs // 2\n",
    "        left_generating_epochs = [2**i for i in range(0, int(np.log2(mid_epoch)) + 1)]\n",
    "        if mid_epoch not in left_generating_epochs:\n",
    "            left_generating_epochs.append(mid_epoch)\n",
    "\n",
    "        right_generating_epochs = [1 + epochs - e for e in left_generating_epochs][::-1]\n",
    "        return left_generating_epochs + right_generating_epochs"
   ],
   "id": "c3bcded08b52ca33",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:18.823452Z",
     "start_time": "2025-08-28T09:13:18.778272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 50\n",
    "poetry_callback = PoetryGenerateCallback(epochs)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    train_sequences,\n",
    "    target_sequences,\n",
    "    batch_size=256,\n",
    "    epochs=epochs,\n",
    "    callbacks=[],\n",
    "    verbose=2\n",
    ")"
   ],
   "id": "9942b787f254ced1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:18.855596Z",
     "start_time": "2025-08-28T09:13:18.852097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the generated poems for analysis\n",
    "# poetry_callback.generated_poems"
   ],
   "id": "f4fbfd0e7777f8d0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:13:19.046385Z",
     "start_time": "2025-08-28T09:13:18.884748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the vocabulary and model\n",
    "\n",
    "vocab = tv.get_vocabulary()\n",
    "with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "    for token in tv.get_vocabulary():\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "model.save(MODEL_PATH)"
   ],
   "id": "73e41d22c225ca47",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Using the model to generate poems",
   "id": "dada31118ce72d4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:29:22.407064Z",
     "start_time": "2025-08-28T09:29:22.169610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload the model\n",
    "\n",
    "with open(VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "tv = layers.TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    standardize=None,\n",
    "    split=None, # 直接喂二维数组\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=24\n",
    ")\n",
    "tv.set_vocabulary(vocab)\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH, compile=False)"
   ],
   "id": "76edf563676fd6c1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T09:29:32.131333Z",
     "start_time": "2025-08-28T09:29:24.160914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "poems = PoetryGenerateCallback.generate_poems()\n",
    "PoetryGenerateCallback.print_poems(poems)"
   ],
   "id": "d1d91eaf6923169e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature 0.0: 海外粹拊揵修腪碑浼顸蔽荡巂蝼薯洵呼呼呼呼呼伧伧菶\n",
      "temperature 0.5: 海外革又饥朦摐问雾盘赖沫斥缈威督厩袛欠裆饤姗馥泳\n",
      "temperature 1.0: 海外颇酎杂夷铸宣蒲抖蒟尝侑梢愿肢瞻鸳嗷当刀雩柿羲\n",
      "temperature 1.5: 海外驱颇绒哙䕟细稑收寇谇葩鹖郜拣蠋鹉课逭脑籥指鍪\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
