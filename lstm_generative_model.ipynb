{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:42.456216Z",
     "start_time": "2025-09-02T03:11:42.449407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from poem.genre import Genre\n",
    "\n",
    "dataset_directory = 'data/Poetry/诗歌数据集'\n",
    "\n",
    "current_genre = Genre.WUJUE\n",
    "# current_genre = Genre.QIJUE\n",
    "# current_genre = Genre.WULV\n",
    "# current_genre = Genre.QILV"
   ],
   "id": "b51a9c0b361327b8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:42.474686Z",
     "start_time": "2025-09-02T03:11:42.470290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all file nams\n",
    "import os\n",
    "\n",
    "def get_all_files(base_dir):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "base_dir = os.path.expanduser(dataset_directory)\n",
    "poem_files = get_all_files(base_dir) # format #{dataset_directory}/XXX.txt"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define the checking functions for single poem file",
   "id": "1d007c4e10e03dd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:42.485482Z",
     "start_time": "2025-09-02T03:11:42.482003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select one file to demonstrate\n",
    "demo_file = poem_files[16]"
   ],
   "id": "17e102c0f159a7e0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:43.703001Z",
     "start_time": "2025-09-02T03:11:42.720953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read content for one file\n",
    "#\n",
    "# Read it to pd.Series and only extract the content\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_file_to_pandas(file_path: str, genre_name: str):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File {file_path} does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    filter_by_genre = df[df[\"体裁\"].astype(str).str.contains(genre_name, na=False)].copy()\n",
    "    poems = filter_by_genre['内容']\n",
    "    return poems\n",
    "\n",
    "one_dynasty_poems = read_file_to_pandas(demo_file, current_genre.genre_name)\n",
    "one_dynasty_poems.shape"
   ],
   "id": "47bce42d13398217",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2711,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:43.730907Z",
     "start_time": "2025-09-02T03:11:43.721150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define some checking functions for the single poem text\n",
    "\n",
    "DEFAULT_VALID_PUNCTATIONS = set(\"！，？。\")\n",
    "\n",
    "def check_poem_punctuation(text: str, positions: list[int], valid_punctuations=None) -> bool:\n",
    "    \"\"\"\n",
    "    Check the punctuation in the fixed positions of the single poem text.\n",
    "\n",
    "    :param text: string, the single poem text\n",
    "    :param positions: list, the fixed positions to check\n",
    "    :param valid_punctuations: set, optional, the valid punctuation characters\n",
    "\n",
    "    :return: bool, whether the text is valid. If False, it means there are other characters\n",
    "             found in the fixed positions.\n",
    "    \"\"\"\n",
    "    if valid_punctuations is None:\n",
    "        valid_punctuations = DEFAULT_VALID_PUNCTATIONS\n",
    "\n",
    "    chars_at_fixed_positions = set(text[i] for i in positions if i < len(text))\n",
    "    invalid_chars = chars_at_fixed_positions - valid_punctuations\n",
    "    return not invalid_chars\n",
    "\n",
    "def check_poem_length(text: str, expected_length=24) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the poem text has the expected length.\n",
    "\n",
    "    :param text: string, the single poem text\n",
    "    :param expected_length: int, the expected length of the poem text\n",
    "\n",
    "    :return: bool, whether the text has the expected length\n",
    "    \"\"\"\n",
    "    return len(text) == expected_length\n",
    "\n",
    "def check_poem(text: str, rows: int, cols: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the poem text is valid by checking both punctuation and length.\n",
    "\n",
    "    :param text: string, the single poem text\n",
    "    :param rows: int, number of rows in the poem.\n",
    "                 E.g. 4 for 绝句\n",
    "    :param cols: int, number of columns in the single poem row，not including the punctuation.\n",
    "                 E.g. 5 for 五言\n",
    "\n",
    "    :return: bool, whether the text is valid\n",
    "    \"\"\"\n",
    "\n",
    "    # At an unknown scene, some items are not string, e.g. float('nan')\n",
    "    if type(text) is not str:\n",
    "        return False\n",
    "\n",
    "    punctuation_positions = [(i + 1) * (cols + 1) - 1 for i in range(rows)]\n",
    "    poem_length = rows * (cols + 1)\n",
    "\n",
    "    return len(text) == poem_length and check_poem_punctuation(text, punctuation_positions)"
   ],
   "id": "5c460c503e773aac",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:43.811564Z",
     "start_time": "2025-09-02T03:11:43.807071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstrate the checking functions on one poem text\n",
    "check_poem(one_dynasty_poems.iloc[0], current_genre.rows, current_genre.cols)"
   ],
   "id": "f2ed76d3bcef53d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:43.916928Z",
     "start_time": "2025-09-02T03:11:43.902308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstrate the checking functions\n",
    "from functools import partial\n",
    "\n",
    "def check_poems(poem_texts: pd.Series, genre: Genre):\n",
    "    \"\"\"\n",
    "    Check the poems Series using the above checking functions and return the mask values (True/False) for each poem text.\n",
    "\n",
    "    :param poem_texts: pd.Series, the series of poem texts, it supports MultiIndex\n",
    "    :param genre: Genre(Enum), the genre rule applied to check\n",
    "    :return: pd.Series of bool, the mask values for each poem text, with the same index as input\n",
    "    \"\"\"\n",
    "    check_for_current_genre = partial(check_poem, rows=genre.rows, cols=genre.cols)\n",
    "    mask = poem_texts.str[:genre.length].apply(check_for_current_genre)\n",
    "    return mask\n",
    "\n",
    "def report_check_results(mask: pd.Series):\n",
    "    return len(mask), mask.sum(), f\"{mask.mean() * 100:.2f}%\"\n",
    "\n",
    "mask = check_poems(one_dynasty_poems, current_genre)\n",
    "report_check_results(mask)"
   ],
   "id": "f288b09bf3b0939a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2711, 2710, '99.96%')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply the checking functions to all poem files",
   "id": "bd4eb2ab2ea08568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:48.725270Z",
     "start_time": "2025-09-02T03:11:43.945305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read all files into pandas Series, with MultiIndex\n",
    "\n",
    "def extract_dynasty_from_filename(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the dynasty from the file name.\n",
    "\n",
    "    :param file_path: string, the file path\n",
    "\n",
    "    :return: string, the dynasty extracted from the file name\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(file_path)\n",
    "    dynasty = base_name.split('.')[0]\n",
    "    return dynasty\n",
    "\n",
    "# poem_files: list[str], format #{dataset_directory}/XXX.txt\n",
    "\n",
    "# Read all files\n",
    "list_of_poems = [read_file_to_pandas(file_path, current_genre.genre_name)  # with default genre\n",
    "                 for file_path in poem_files]\n",
    "\n",
    "# Extract all dynasties from file names\n",
    "dynasty_list = [extract_dynasty_from_filename(file) for file in poem_files]\n",
    "\n",
    "# Merge them into MultiIndex DataFrame\n",
    "all_dynasty_poems = pd.concat(list_of_poems, keys=dynasty_list)"
   ],
   "id": "f71ab06ee9bc1112",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:48.841543Z",
     "start_time": "2025-09-02T03:11:48.740284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the checking functions to all dynasty poems\n",
    "mask = check_poems(all_dynasty_poems, current_genre)\n",
    "report_check_results(mask)"
   ],
   "id": "bcab06e439887adc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37081, 37012, '99.81%')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:48.881169Z",
     "start_time": "2025-09-02T03:11:48.868778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the cleaned dataset using mask values\n",
    "cleaned_poems = all_dynasty_poems[mask].str[:current_genre.length]\n",
    "cleaned_poems.shape"
   ],
   "id": "78b4175e909df1a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37012,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build the text_vectorization",
   "id": "f28f6a443366568f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:59.847147Z",
     "start_time": "2025-09-02T03:11:48.954276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import layers\n",
    "\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    split='character',\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=current_genre.length\n",
    ")\n",
    "text_vectorization.adapt(cleaned_poems)"
   ],
   "id": "a220c7acb990d268",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 11:11:52.571937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:11:59.932459Z",
     "start_time": "2025-09-02T03:11:59.863336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstrate the text_vectorization\n",
    "\n",
    "# Print the vocabulary\n",
    "print('Vocabulary size:', text_vectorization.vocabulary_size())\n",
    "print('Vocabulary samples:', ''.join(text_vectorization.get_vocabulary()[:20]))\n",
    "\n",
    "# Encode\n",
    "encoded = text_vectorization(cleaned_poems.iloc[0])\n",
    "\n",
    "# Decode\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "decoded =[vocabulary[i] for i in encoded]\n",
    "print('Encoded:', encoded.numpy())\n",
    "print('Decoded:', ''.join(decoded))"
   ],
   "id": "3aa5eaf2598f53af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6350\n",
      "Vocabulary samples: [UNK]，。不人山风一花无来何云有日月春水中\n",
      "Encoded: [1152 1152  948  466   65    2 1074  177  740   41  604    3 1103   64\n",
      "    6   16 1230    2  945 2176  183    7   23    3]\n",
      "Decoded: 脉脉广川流，驱马历长洲。鹊飞山月曙，蝉噪野风秋。\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:12:00.363209Z",
     "start_time": "2025-09-02T03:11:59.988740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode all the poems\n",
    "train_token_ids = text_vectorization(cleaned_poems)\n",
    "print('shape of train dataset:', train_token_ids.shape)"
   ],
   "id": "6b3f40bf5f22b1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train dataset: (37012, 24)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:12:00.572128Z",
     "start_time": "2025-09-02T03:12:00.384652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we can save the vocabulary of text_vectorization\n",
    "!mkdir -p models\n",
    "\n",
    "def save_vocabulary(vocabulary: list[str], genre: Genre):\n",
    "    vocab_path = f'models/{genre.genre_name}_vocabulary.txt'\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        for token in vocabulary:\n",
    "            f.write(f\"{token}\\n\")\n",
    "\n",
    "save_vocabulary(vocabulary, current_genre)"
   ],
   "id": "101177fa97208ed3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate the model using LSTM layer",
   "id": "5953680e1a29306e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:12:00.611650Z",
     "start_time": "2025-09-02T03:12:00.595482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the train dataset\n",
    "train_sequences = train_token_ids[:, :-1]\n",
    "target_sequences = train_token_ids[:, 1:]\n",
    "\n",
    "train_sequences.shape, target_sequences.shape"
   ],
   "id": "a1fda5cc4bcae125",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([37012, 23]), TensorShape([37012, 23]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:12:00.939599Z",
     "start_time": "2025-09-02T03:12:00.670671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build a simple LSTM Decoder model\n",
    "\n",
    "import keras\n",
    "from keras import models, layers\n",
    "\n",
    "class Config:\n",
    "    batch_size = 256\n",
    "    epochs = 50\n",
    "    vocab_size = len(vocabulary)\n",
    "    embedding_dim = 100\n",
    "    lstm_units = 512\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "def build_model(config: Config) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"inputs\")\n",
    "    x = layers.Embedding(input_dim=config.vocab_size, output_dim=config.embedding_dim, name=\"embedding\")(inputs)\n",
    "    x = layers.LSTM(config.lstm_units, return_sequences=True, name=\"lstm\")(x)\n",
    "    x = layers.Dropout(config.dropout_rate, name=\"dropout\")(x)\n",
    "    outputs = layers.Dense(config.vocab_size, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs, name=\"lstm_decoder\")\n",
    "\n",
    "config = Config()\n",
    "model = build_model(config)\n",
    "model.summary()"
   ],
   "id": "1a34840947911408",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"lstm_decoder\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"lstm_decoder\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (\u001B[38;5;33mInputLayer\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m)      │       \u001B[38;5;34m635,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │     \u001B[38;5;34m1,255,424\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001B[38;5;33mDense\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6350\u001B[0m)     │     \u001B[38;5;34m3,257,550\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">635,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,255,424</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6350</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,257,550</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m5,147,974\u001B[0m (19.64 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,147,974</span> (19.64 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m5,147,974\u001B[0m (19.64 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,147,974</span> (19.64 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:13:33.844905Z",
     "start_time": "2025-09-02T03:12:01.022613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    train_sequences,\n",
    "    target_sequences,\n",
    "    batch_size=config.batch_size,\n",
    "    epochs=config.epochs,\n",
    "    callbacks=[],\n",
    "    verbose=2\n",
    ")"
   ],
   "id": "19774448c55da8a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m model.compile(\n\u001B[32m      2\u001B[39m     loss=\u001B[33m\"\u001B[39m\u001B[33msparse_categorical_crossentropy\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      3\u001B[39m     optimizer=\u001B[33m\"\u001B[39m\u001B[33madam\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      4\u001B[39m     metrics=[\u001B[33m\"\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m      5\u001B[39m )\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_sequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget_sequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\n\u001B[32m     13\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    115\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001B[39m, in \u001B[36mTensorFlowTrainer.fit\u001B[39m\u001B[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[39m\n\u001B[32m    375\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[32m    376\u001B[39m     callbacks.on_train_batch_begin(step)\n\u001B[32m--> \u001B[39m\u001B[32m377\u001B[39m     logs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    378\u001B[39m     callbacks.on_train_batch_end(step, logs)\n\u001B[32m    379\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stop_training:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001B[39m, in \u001B[36mTensorFlowTrainer._make_function.<locals>.function\u001B[39m\u001B[34m(iterator)\u001B[39m\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfunction\u001B[39m(iterator):\n\u001B[32m    217\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[32m    218\u001B[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001B[32m    219\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m         opt_outputs = \u001B[43mmulti_step_on_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_outputs.has_value():\n\u001B[32m    222\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[39m, in \u001B[36mFunction.__call__\u001B[39m\u001B[34m(self, *args, **kwds)\u001B[39m\n\u001B[32m    830\u001B[39m compiler = \u001B[33m\"\u001B[39m\u001B[33mxla\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mnonXla\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    832\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m._jit_compile):\n\u001B[32m--> \u001B[39m\u001B[32m833\u001B[39m   result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    835\u001B[39m new_tracing_count = \u001B[38;5;28mself\u001B[39m.experimental_get_tracing_count()\n\u001B[32m    836\u001B[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001B[39m, in \u001B[36mFunction._call\u001B[39m\u001B[34m(self, *args, **kwds)\u001B[39m\n\u001B[32m    875\u001B[39m \u001B[38;5;28mself\u001B[39m._lock.release()\n\u001B[32m    876\u001B[39m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[32m    877\u001B[39m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m878\u001B[39m results = \u001B[43mtracing_compilation\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    881\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._created_variables:\n\u001B[32m    882\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mCreating variables on a non-first call to a function\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    883\u001B[39m                    \u001B[33m\"\u001B[39m\u001B[33m decorated with tf.function.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[39m, in \u001B[36mcall_function\u001B[39m\u001B[34m(args, kwargs, tracing_options)\u001B[39m\n\u001B[32m    137\u001B[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001B[32m    138\u001B[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[32m    140\u001B[39m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001B[39m, in \u001B[36mConcreteFunction._call_flat\u001B[39m\u001B[34m(self, tensor_inputs, captured_inputs)\u001B[39m\n\u001B[32m   1318\u001B[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001B[32m   1319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001B[32m   1320\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[32m   1321\u001B[39m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inference_function\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1323\u001B[39m forward_backward = \u001B[38;5;28mself\u001B[39m._select_forward_and_backward_functions(\n\u001B[32m   1324\u001B[39m     args,\n\u001B[32m   1325\u001B[39m     possible_gradient_type,\n\u001B[32m   1326\u001B[39m     executing_eagerly)\n\u001B[32m   1327\u001B[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[39m, in \u001B[36mAtomicFunction.call_preflattened\u001B[39m\u001B[34m(self, args)\u001B[39m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core.Tensor]) -> Any:\n\u001B[32m    215\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m   flat_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.function_type.pack_output(flat_outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[39m, in \u001B[36mAtomicFunction.call_flat\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m record.stop_recording():\n\u001B[32m    250\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._bound_context.executing_eagerly():\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_bound_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction_type\u001B[49m\u001B[43m.\u001B[49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    256\u001B[39m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    257\u001B[39m     outputs = make_call_op_in_graph(\n\u001B[32m    258\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    259\u001B[39m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[32m    260\u001B[39m         \u001B[38;5;28mself\u001B[39m._bound_context.function_call_options.as_attrs(),\n\u001B[32m    261\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001B[39m, in \u001B[36mContext.call_function\u001B[39m\u001B[34m(self, name, tensor_inputs, num_outputs)\u001B[39m\n\u001B[32m   1498\u001B[39m cancellation_context = cancellation.context()\n\u001B[32m   1499\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1500\u001B[39m   outputs = \u001B[43mexecute\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1501\u001B[39m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1502\u001B[39m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1503\u001B[39m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1504\u001B[39m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1505\u001B[39m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1506\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1507\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1508\u001B[39m   outputs = execute.execute_with_cancellation(\n\u001B[32m   1509\u001B[39m       name.decode(\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   1510\u001B[39m       num_outputs=num_outputs,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1514\u001B[39m       cancellation_manager=cancellation_context,\n\u001B[32m   1515\u001B[39m   )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/nlpia/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001B[39m, in \u001B[36mquick_execute\u001B[39m\u001B[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     52\u001B[39m   ctx.ensure_initialized()\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m   tensors = \u001B[43mpywrap_tfe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m core._NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     56\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:13:33.871961Z",
     "start_time": "2025-09-01T13:19:21.424842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained model\n",
    "model_path = f'models/{current_genre.genre_name}_lstm_model-epoch{config.epochs}.keras'\n",
    "model.save(model_path)"
   ],
   "id": "d622829ae544cb49",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate the text using trained model",
   "id": "6a569a8b816c0206"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T03:16:50.813207Z",
     "start_time": "2025-09-02T03:16:48.432283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstrate text generation using PoemGenerator class\n",
    "\n",
    "from poem.generator import PoemGenerator\n",
    "\n",
    "poem_generator = PoemGenerator(\n",
    "    vectorization_model=text_vectorization,\n",
    "    generation_model=model,\n",
    "    genre=current_genre\n",
    ")\n",
    "\n",
    "poem_generator.generate(\"海外\", temperature=0)"
   ],
   "id": "2ea6ec872ab0f21a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'海外。。。。。。。。。。。。。。。。。。。。。。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
