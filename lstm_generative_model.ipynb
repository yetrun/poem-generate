{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Use LSTM to generate the poem\n",
    "\n",
    "The procedure:\n",
    "\n",
    "1. Embedding Layer;\n",
    "2. LSTM Decoder-only;\n",
    "3. Sample for generation;\n",
    "4. Use all data to train, which overfits the training data."
   ],
   "id": "67d05df12625237f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Download The Dataset",
   "id": "243f99368fd6af89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optional: set the proxy\n",
    "%env all_proxy=socks5://127.0.0.1:7897\n",
    "\n",
    "!mkdir -p data\n",
    "!git clone https://github.com/xiu-ze/Poetry.git data/Poetry"
   ],
   "id": "71814b0fc679536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T09:04:23.664724Z",
     "start_time": "2025-08-20T09:04:23.653988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all files\n",
    "import os\n",
    "\n",
    "def get_all_files(base_dir):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "base_dir = os.path.expanduser('data/Poetry/诗歌数据集')\n",
    "poem_files = get_all_files(base_dir)\n",
    "poem_files"
   ],
   "id": "8488e291bbcafca6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Poetry/诗歌数据集/秦.csv',\n",
       " 'data/Poetry/诗歌数据集/先秦.csv',\n",
       " 'data/Poetry/诗歌数据集/隋.csv',\n",
       " 'data/Poetry/诗歌数据集/辽.csv',\n",
       " 'data/Poetry/诗歌数据集/当代.csv',\n",
       " 'data/Poetry/诗歌数据集/明_1.csv',\n",
       " 'data/Poetry/诗歌数据集/明_2.csv',\n",
       " 'data/Poetry/诗歌数据集/明_3.csv',\n",
       " 'data/Poetry/诗歌数据集/清_3.csv',\n",
       " 'data/Poetry/诗歌数据集/清_2.csv',\n",
       " 'data/Poetry/诗歌数据集/明_4.csv',\n",
       " 'data/Poetry/诗歌数据集/元.csv',\n",
       " 'data/Poetry/诗歌数据集/清_1.csv',\n",
       " 'data/Poetry/诗歌数据集/南北朝.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_1.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_2.csv',\n",
       " 'data/Poetry/诗歌数据集/唐.csv',\n",
       " 'data/Poetry/诗歌数据集/近现代.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_3.csv',\n",
       " 'data/Poetry/诗歌数据集/汉.csv',\n",
       " 'data/Poetry/诗歌数据集/金.csv',\n",
       " 'data/Poetry/诗歌数据集/魏晋.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Clean the dataset\n",
    "\n",
    "1. Truncate the poems to 24 characters;\n",
    "2. Check the invalid signs in the fixed positions and remove the abnormal items."
   ],
   "id": "67716901997dc298"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T09:48:04.341664Z",
     "start_time": "2025-08-20T09:48:04.336807Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 16,
   "source": [
    "# Read dataset from one file\n",
    "import pandas as pd\n",
    "\n",
    "project_root = os.path.abspath('.')\n",
    "\n",
    "def read_file_to_df(file_path):\n",
    "    file = os.path.join(project_root, file_path)\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File {file} does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    filter_by_wujue = df[df[\"体裁\"].astype(str).str.contains(\"五言绝句\", na=False)].copy()\n",
    "    wujue_content = filter_by_wujue['内容']\n",
    "    return wujue_content"
   ],
   "id": "46885637a1419f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:07:28.546436Z",
     "start_time": "2025-08-20T12:07:28.539400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transform the pandas Series to numpy array and check\n",
    "import numpy as np\n",
    "\n",
    "def check_punctuation(poem_texts, positions=[5, 11, 17, 23]):\n",
    "    \"\"\"\n",
    "    Check the punctuation in the fixed positions of the poem texts.\n",
    "    It prints the found characters in the fixed positions and identifies any invalid characters.\n",
    "\n",
    "    :param poem_texts: np.ndarray, the array of poem texts\n",
    "    :return: set, invalid punctuations\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the fixed location values\n",
    "    chars_at_fixed_positions = set(poem_texts[:, positions].reshape(-1))\n",
    "    print('Characters found at fixed positions:', ''.join(chars_at_fixed_positions))\n",
    "\n",
    "    # Check the invalid characters\n",
    "    valid_chars = set(\"！，？。\")\n",
    "    invalid_chars = chars_at_fixed_positions - valid_chars\n",
    "    print('Find invalid characters:', ''.join(invalid_chars))\n",
    "\n",
    "    return invalid_chars\n",
    "\n",
    "def clean_poem_texts(poem_texts):\n",
    "    \"\"\"\n",
    "    Clean the poem texts by removing invalid characters and truncating to 24 characters.\n",
    "\n",
    "    :param poem_texts: pd.Series, the series of poem texts\n",
    "    :return: ndarray, cleaned poem texts with shape (n, 24)\n",
    "    \"\"\"\n",
    "    # Adjust the size of every poem item to 24\n",
    "    poems_truncated = poem_texts.map(lambda x: x[:24])\n",
    "\n",
    "    # Check the size less than 24\n",
    "    poems_small = poems_truncated[poems_truncated.str.len() < 24]\n",
    "    if not poems_small.empty:\n",
    "        print('Count of poems with size less than 24:', len(poems_small))\n",
    "        poems_truncated = poems_truncated[poems_truncated.str.len() == 24]\n",
    "\n",
    "    # Transform the pandas Series to numpy array\n",
    "    poem_numpy = np.array(\n",
    "        poems_truncated.map(list).to_list()\n",
    "    )\n",
    "    print('shape of numpy array', poem_numpy.shape)\n",
    "\n",
    "    # Check the signs in the wujue array\n",
    "    invalid_chars = check_punctuation(poem_numpy)\n",
    "\n",
    "    # Find the abnormal items\n",
    "    abnormal_items = poem_numpy[\n",
    "        np.isin(poem_numpy[:, [5, 11, 17, 23]], list(invalid_chars)).any(axis=1)\n",
    "    ]\n",
    "    abnormal_count = len(abnormal_items)\n",
    "    print('abnormal count:', abnormal_count)\n",
    "    if abnormal_count > 0:\n",
    "        print('abnormal item: ', ''.join(abnormal_items[0]))\n",
    "\n",
    "    # Remove the abnormal item\n",
    "    poems_removed_invalid = poem_numpy[\n",
    "        ~np.isin(poem_numpy[:, [5, 11, 17, 23]], list(invalid_chars)).any(axis=1)\n",
    "    ]\n",
    "\n",
    "    print('===== After removing the abnormal items =====')\n",
    "    print('shape of numpy array', poems_removed_invalid.shape)\n",
    "    check_punctuation(poems_removed_invalid)\n",
    "\n",
    "    # Convert back to pandas Series\n",
    "    return poems_removed_invalid"
   ],
   "id": "250d55c9d9594e57",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:38:07.868391Z",
     "start_time": "2025-08-20T12:38:00.957905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read all files and clean the dataset\n",
    "\n",
    "all_poems = []\n",
    "\n",
    "for file in poem_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    poem_texts = read_file_to_df(file)\n",
    "    if poem_texts.empty:\n",
    "        print(\"No valid poems found in this file.\\n\")\n",
    "        continue\n",
    "\n",
    "    result = clean_poem_texts(poem_texts)\n",
    "    all_poems.append(result)\n",
    "    print()\n",
    "\n",
    "# Concatenate all cleaned poems into a single array\n",
    "train_poems_numpy = np.concatenate(all_poems, axis=0)\n",
    "train_poems_numpy.shape"
   ],
   "id": "c85748c0068ea4ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data/Poetry/诗歌数据集/秦.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/先秦.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/隋.csv\n",
      "shape of numpy array (71, 24)\n",
      "Characters found at fixed positions: 。，？\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (71, 24)\n",
      "Characters found at fixed positions: 。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/辽.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/当代.csv\n",
      "shape of numpy array (1109, 24)\n",
      "Characters found at fixed positions: 」？轻鼠冕；帽闲宵不文对边山大华歌！栏流。，：传\n",
      "Find invalid characters: 传大轻鼠华冕歌；帽闲宵栏流不文对边：山」\n",
      "abnormal count: 13\n",
      "abnormal item:  琅玕经雨青，染绿山溪水。“萧萧我凭栏”，袅袅清歌\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1096, 24)\n",
      "Characters found at fixed positions: ！。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_1.csv\n",
      "shape of numpy array (3712, 24)\n",
      "Characters found at fixed positions: 月曲。？丛金相过幕兰，人》罗者歌自鹤\n",
      "Find invalid characters: 月自曲丛相金幕兰人鹤罗者歌》过\n",
      "abnormal count: 5\n",
      "abnormal item:  片舫连双桨，呕哑杂哩罗。惯听吴棹曲，羞杀《阆中歌\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3707, 24)\n",
      "Characters found at fixed positions: 。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_2.csv\n",
      "Count of poems with size less than 24: 3\n",
      "shape of numpy array (3829, 24)\n",
      "Characters found at fixed positions: 。水，？\n",
      "Find invalid characters: 水\n",
      "abnormal count: 1\n",
      "abnormal item:  操杵力不任，当垆心自鄙。花时掩关坐，焚香读《秋水\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3828, 24)\n",
      "Characters found at fixed positions: 。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_3.csv\n",
      "shape of numpy array (4249, 24)\n",
      "Characters found at fixed positions: 。？，学木筝！》；传\n",
      "Find invalid characters: 学木筝》；传\n",
      "abnormal count: 6\n",
      "abnormal item:  雨馀鸟语凉，斜阳竹深见。频来非看花，借读《高僧传\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (4243, 24)\n",
      "Characters found at fixed positions: 。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_3.csv\n",
      "shape of numpy array (1882, 24)\n",
      "Characters found at fixed positions: 蛾尔？断销多里釭轻遥天；沾敲尽磨生弱饶乍话3质安人诉得近通！外真日。伸，思亲梅旧说1草撞\n",
      "Find invalid characters: 蛾尔断销多里釭轻遥天；沾敲尽磨生弱饶乍话3质安人诉得近通外真日伸思亲梅旧说1草撞\n",
      "abnormal count: 16\n",
      "abnormal item:  昔子贵之时，曾居洛阳市。洛阳人情何？曰『薄乎云尔\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1866, 24)\n",
      "Characters found at fixed positions: ！。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_2.csv\n",
      "shape of numpy array (1473, 24)\n",
      "Characters found at fixed positions: 曲。？，！；有\n",
      "Find invalid characters: ；曲有\n",
      "abnormal count: 4\n",
      "abnormal item:  闻君向南投，曾过北投宿。试从鹿水头，一续「乌溪曲\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1469, 24)\n",
      "Characters found at fixed positions: ！。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_4.csv\n",
      "shape of numpy array (2184, 24)\n",
      "Characters found at fixed positions: 。？皇，天\n",
      "Find invalid characters: 皇天\n",
      "abnormal count: 1\n",
      "abnormal item:  去去去此间，不是留侬处。侬住三十三天天外天，玉皇\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2183, 24)\n",
      "Characters found at fixed positions: 。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/元.csv\n",
      "shape of numpy array (2624, 24)\n",
      "Characters found at fixed positions: 曲？相广枝篇赋痕经扇歌！两苧浮。，南梅海：星\n",
      "Find invalid characters: 曲相经广枝篇赋歌两苧浮南星梅海：扇痕\n",
      "abnormal count: 9\n",
      "abnormal item:  老竹空岩里，悬厓飞水前。欲识逍遥境，试读《逍遥篇\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2615, 24)\n",
      "Characters found at fixed positions: ！。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_1.csv\n",
      "shape of numpy array (1711, 24)\n",
      "Characters found at fixed positions: 。？！城水人，潮杜箫；鹤\n",
      "Find invalid characters: 城水潮人杜箫；鹤\n",
      "abnormal count: 4\n",
      "abnormal item:  昔作秦淮客，朱楼赋《洞箫》。白头故人尽，重上石城\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1707, 24)\n",
      "Characters found at fixed positions: ！。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/南北朝.csv\n",
      "shape of numpy array (1, 24)\n",
      "Characters found at fixed positions: 。，\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1, 24)\n",
      "Characters found at fixed positions: 。，\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_1.csv\n",
      "shape of numpy array (4397, 24)\n",
      "Characters found at fixed positions: 。，；莲前桨\n",
      "Find invalid characters: 桨莲；前\n",
      "abnormal count: 2\n",
      "abnormal item:  晚过鸳鸯浦，无心唱《采莲》。莫嗔兰桨急，为要趁前\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (4395, 24)\n",
      "Characters found at fixed positions: 。，\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_2.csv\n",
      "shape of numpy array (3470, 24)\n",
      "Characters found at fixed positions: 。，\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3470, 24)\n",
      "Characters found at fixed positions: 。，\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/唐.csv\n",
      "shape of numpy array (2711, 24)\n",
      "Characters found at fixed positions: 沽。？，此！\n",
      "Find invalid characters: 沽此\n",
      "abnormal count: 1\n",
      "abnormal item:  勒马问樵夫，前村酒有无。「杜康家在此，一任君来沽\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2710, 24)\n",
      "Characters found at fixed positions: ！。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/近现代.csv\n",
      "shape of numpy array (398, 24)\n",
      "Characters found at fixed positions: 。？，！；\n",
      "Find invalid characters: ；\n",
      "abnormal count: 1\n",
      "abnormal item:  明月照秋霜，今朝还故乡；留得头颅在，雄心誓不降。\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (397, 24)\n",
      "Characters found at fixed positions: ！。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_3.csv\n",
      "shape of numpy array (3005, 24)\n",
      "Characters found at fixed positions: 曲个。？花懒，灯\n",
      "Find invalid characters: 曲个花懒灯\n",
      "abnormal count: 3\n",
      "abnormal item:  明月入我池，皎皎铺纻缟。何日变成缁？《太玄》吾懒\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3002, 24)\n",
      "Characters found at fixed positions: 。，？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/汉.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/金.csv\n",
      "shape of numpy array (252, 24)\n",
      "Characters found at fixed positions: 。，\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (252, 24)\n",
      "Characters found at fixed positions: 。，\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/魏晋.csv\n",
      "No valid poems found in this file.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37012, 24)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Dataset to token id sequences",
   "id": "44dc0adc2c6c2704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T14:42:48.788888Z",
     "start_time": "2025-08-20T14:42:48.542030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import layers\n",
    "\n",
    "tv = layers.TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    standardize=None,\n",
    "    split=None, # 直接喂二维数组\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=24\n",
    ")\n",
    "tv.adapt(train_poems_numpy)"
   ],
   "id": "c890cc6fbcb7117",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T14:43:00.931899Z",
     "start_time": "2025-08-20T14:43:00.885884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demo usage of tv\n",
    "\n",
    "# Print the vocabulary\n",
    "print('Vocabulary size:', tv.vocabulary_size())\n",
    "print('Vocabulary:', ''.join(tv.get_vocabulary()[:20]))\n",
    "\n",
    "# Encode\n",
    "encoded = tv(train_poems_numpy[0])\n",
    "\n",
    "# Decode\n",
    "vocab = tv.get_vocabulary()\n",
    "decoded =[vocab[i] for i in encoded]\n",
    "print('Encoded:', encoded.numpy())\n",
    "print('Decoded:', ''.join(decoded))"
   ],
   "id": "ef849f42f169ce1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6350\n",
      "Vocabulary: [UNK]，。不人山风一花无来何云有日月春水中\n",
      "Encoded: [1152 1152  948  466   65    2 1074  177  740   41  604    3 1103   64\n",
      "    6   16 1230    2  945 2176  183    7   23    3]\n",
      "Decoded: 脉脉广川流，驱马历长洲。鹊飞山月曙，蝉噪野风秋。\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T14:43:13.512879Z",
     "start_time": "2025-08-20T14:43:13.313307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode all the poems\n",
    "train_token_ids = tv(train_poems_numpy)\n",
    "print('shape of wujue_token_ids:', train_token_ids.shape)"
   ],
   "id": "8277b7ae292b1f71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of wujue_token_ids: (37012, 24)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Build the LSTM Decoder model",
   "id": "6145e1fc59ad943a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:42:17.179678Z",
     "start_time": "2025-08-20T12:42:17.165109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the train dataset\n",
    "train_sequences = train_token_ids[:, :-1]\n",
    "target_sequences = train_token_ids[:, 1:]\n",
    "\n",
    "train_sequences.shape, target_sequences.shape"
   ],
   "id": "43b8b72f51002c3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([37012, 23]), TensorShape([37012, 23]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:42:21.011282Z",
     "start_time": "2025-08-20T12:42:20.908724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build a simple LSTM Decoder model\n",
    "\n",
    "import keras\n",
    "from keras import models, layers\n",
    "\n",
    "def build_model(vocab_size):\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"inputs\")\n",
    "    x_embedded = layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=100, name=\"embedding\"\n",
    "    )(inputs)\n",
    "    x_lstm_output = layers.LSTM(\n",
    "        128, return_sequences=True, name=\"lstm\"\n",
    "    )(x_embedded)\n",
    "    outputs = layers.Dense(\n",
    "        vocab_size, activation=\"softmax\", name=\"output\"\n",
    "    )(x_lstm_output)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs, name=\"lstm_decoder\")\n",
    "\n",
    "model = build_model(tv.vocabulary_size())\n",
    "model.summary()\n"
   ],
   "id": "75954408bd3f32c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"lstm_decoder\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"lstm_decoder\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (\u001B[38;5;33mInputLayer\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m)      │       \u001B[38;5;34m635,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)      │       \u001B[38;5;34m117,248\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001B[38;5;33mDense\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6350\u001B[0m)     │       \u001B[38;5;34m819,150\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">635,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6350</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,150</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,571,398\u001B[0m (5.99 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,571,398</span> (5.99 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,571,398\u001B[0m (5.99 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,571,398</span> (5.99 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T14:50:52.139495Z",
     "start_time": "2025-08-20T14:50:49.951426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the sample generate function\n",
    "def generate(prompt, max_length=24, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate a poem based on the start prompt\n",
    "\n",
    "    Returns:\n",
    "        A generated poem as a string.\n",
    "    \"\"\"\n",
    "    prompt_inputs = list(prompt)\n",
    "    generated = tv(prompt_inputs)[:len(prompt)].numpy().tolist()\n",
    "    while len(generated) < max_length:\n",
    "        input_sequence = np.array(generated).reshape(1, -1)\n",
    "        predictions = model.predict(input_sequence, verbose=0)[0]\n",
    "        next_token_id = sample(predictions[-1], temperature)\n",
    "        generated.append(next_token_id)\n",
    "    return ''.join(tv.get_vocabulary()[token_id] for token_id in generated)\n",
    "\n",
    "def sample(predictions, temperature=1.0, eps1=1e-20, eps2=1e-9):\n",
    "    p = np.asarray(predictions, dtype=np.float64)\n",
    "\n",
    "    # The two key points: log(p + eps1) divide by (T + eps2)\n",
    "    logits = np.log(p + eps1) / (float(temperature) + eps2)\n",
    "\n",
    "    # Subtract the max logit to prevent overflow\n",
    "    logits -= np.max(logits)\n",
    "\n",
    "    q = np.exp(logits)\n",
    "    q /= q.sum()\n",
    "    return int(np.random.choice(len(q), p=q))\n",
    "\n",
    "\n",
    "generate(\"海外\", temperature=0)"
   ],
   "id": "2145bf31d5cc14b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'海外春风雨，山花一叶开。不知春水上，不见一枝花。'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T14:58:30.255200Z",
     "start_time": "2025-08-20T14:58:30.245691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define callback to print the sample generative poem every 10 epochs\n",
    "end_epoch = 10\n",
    "\n",
    "class PoetryGenerateCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.next_print_epoch = 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch += 1\n",
    "        if epoch != self.next_print_epoch and epoch != end_epoch:\n",
    "            return\n",
    "\n",
    "        print(f\"Generating poems at epoch {epoch}:\\n\")\n",
    "        self._print_generated_poems()\n",
    "        self.next_print_epoch *= 2\n",
    "\n",
    "    @staticmethod\n",
    "    def _print_generated_poems():\n",
    "        temperatures = [0, 0.5, 1.0, 1.5]\n",
    "        generated_texts = [\n",
    "            generate('海外', max_length=24, temperature=temp)\n",
    "            for temp in temperatures\n",
    "        ]\n",
    "\n",
    "        for temp, text in zip(temperatures, generated_texts):\n",
    "            print(f\"temperature {temp}:{text}\\n\")"
   ],
   "id": "c3bcded08b52ca33",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T15:01:18.236238Z",
     "start_time": "2025-08-20T14:58:42.004862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    train_sequences,\n",
    "    target_sequences,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    callbacks=[PoetryGenerateCallback()],\n",
    "    verbose=2\n",
    ")"
   ],
   "id": "9942b787f254ced1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating poems at epoch 1:\n",
      "\n",
      "temperature 0:海外春风起，山花一叶开。不知春色里，不见一枝花。\n",
      "\n",
      "temperature 0.5:海外青江水，山舟雁已多。青风吹不尽，不得一枝红。\n",
      "\n",
      "temperature 1.0:海外采嶂冷，濯旗苍草流。非织彫陈在，佛下春亭城。\n",
      "\n",
      "temperature 1.5:海外甘污拙，计随曹荐琴。兼杰卢沟始，羞橙九月中。\n",
      "\n",
      "579/579 - 156s - 270ms/step - accuracy: 0.2378 - loss: 4.9476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x146f78440>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Using the model to generate poems",
   "id": "dada31118ce72d4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T15:01:28.418700Z",
     "start_time": "2025-08-20T15:01:26.742856Z"
    }
   },
   "cell_type": "code",
   "source": "generate('海外')",
   "id": "3fdb53c883b8a409",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'海外安来日，春风伴菜间。水茂宋鞭发，多久坐钓香。'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
