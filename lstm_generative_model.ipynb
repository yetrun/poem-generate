{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Use LSTM to generate the poem\n",
    "\n",
    "The procedure:\n",
    "\n",
    "1. Embedding Layer;\n",
    "2. LSTM Decoder-only;\n",
    "3. Sample for generation;\n",
    "4. Use all data to train, which overfits the training data."
   ],
   "id": "67d05df12625237f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:08.931724Z",
     "start_time": "2025-08-28T13:00:08.892397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%script echo skipping\n",
    "\n",
    "%pip install pandas"
   ],
   "id": "5cd8e1b35e1257b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:08.964499Z",
     "start_time": "2025-08-28T13:00:08.958470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "VOCAB_PATH = os.environ.get(\"POETRY_VOCAB_PATH\", \"models/poetry_vocabulary.txt\")\n",
    "MODEL_PATH = os.environ.get(\"POETRY_MODEL_PATH\", \"models/lstm_poetry_model.keras\")"
   ],
   "id": "948bb3d95154d1a9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Download The Dataset",
   "id": "243f99368fd6af89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:09.059889Z",
     "start_time": "2025-08-28T13:00:09.057029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Optional: set the proxy\n",
    "%env all_proxy=socks5://127.0.0.1:7897\n",
    "\n",
    "!mkdir -p data\n",
    "!git clone https://github.com/xiu-ze/Poetry.git data/Poetry"
   ],
   "id": "71814b0fc679536",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:09.176043Z",
     "start_time": "2025-08-28T13:00:09.160066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all files\n",
    "import os\n",
    "\n",
    "def get_all_files(base_dir):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "base_dir = os.path.expanduser('data/Poetry/诗歌数据集')\n",
    "poem_files = get_all_files(base_dir)\n",
    "poem_files"
   ],
   "id": "8488e291bbcafca6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Poetry/诗歌数据集/秦.csv',\n",
       " 'data/Poetry/诗歌数据集/先秦.csv',\n",
       " 'data/Poetry/诗歌数据集/隋.csv',\n",
       " 'data/Poetry/诗歌数据集/辽.csv',\n",
       " 'data/Poetry/诗歌数据集/当代.csv',\n",
       " 'data/Poetry/诗歌数据集/明_1.csv',\n",
       " 'data/Poetry/诗歌数据集/明_2.csv',\n",
       " 'data/Poetry/诗歌数据集/明_3.csv',\n",
       " 'data/Poetry/诗歌数据集/清_3.csv',\n",
       " 'data/Poetry/诗歌数据集/清_2.csv',\n",
       " 'data/Poetry/诗歌数据集/明_4.csv',\n",
       " 'data/Poetry/诗歌数据集/元.csv',\n",
       " 'data/Poetry/诗歌数据集/清_1.csv',\n",
       " 'data/Poetry/诗歌数据集/南北朝.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_1.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_2.csv',\n",
       " 'data/Poetry/诗歌数据集/唐.csv',\n",
       " 'data/Poetry/诗歌数据集/近现代.csv',\n",
       " 'data/Poetry/诗歌数据集/宋_3.csv',\n",
       " 'data/Poetry/诗歌数据集/汉.csv',\n",
       " 'data/Poetry/诗歌数据集/金.csv',\n",
       " 'data/Poetry/诗歌数据集/魏晋.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Clean the dataset\n",
    "\n",
    "1. Truncate the poems to 24 characters;\n",
    "2. Check the invalid signs in the fixed positions and remove the abnormal items."
   ],
   "id": "67716901997dc298"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:09.473954Z",
     "start_time": "2025-08-28T13:00:09.267258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read dataset from one file\n",
    "import pandas as pd\n",
    "\n",
    "project_root = os.path.abspath('.')\n",
    "\n",
    "def read_file_to_df(file_path):\n",
    "    file = os.path.join(project_root, file_path)\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File {file} does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    filter_by_wujue = df[df[\"体裁\"].astype(str).str.contains(\"五言绝句\", na=False)].copy()\n",
    "    wujue_content = filter_by_wujue['内容']\n",
    "    return wujue_content"
   ],
   "id": "46885637a1419f00",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:09.522623Z",
     "start_time": "2025-08-28T13:00:09.510317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transform the pandas Series to numpy array and check\n",
    "import numpy as np\n",
    "\n",
    "def check_punctuation(poem_texts, positions=[5, 11, 17, 23]):\n",
    "    \"\"\"\n",
    "    Check the punctuation in the fixed positions of the poem texts.\n",
    "    It prints the found characters in the fixed positions and identifies any invalid characters.\n",
    "\n",
    "    :param poem_texts: np.ndarray, the array of poem texts\n",
    "    :return: set, invalid punctuations\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the fixed location values\n",
    "    chars_at_fixed_positions = set(poem_texts[:, positions].reshape(-1))\n",
    "    print('Characters found at fixed positions:', ''.join(chars_at_fixed_positions))\n",
    "\n",
    "    # Check the invalid characters\n",
    "    valid_chars = set(\"！，？。\")\n",
    "    invalid_chars = chars_at_fixed_positions - valid_chars\n",
    "    print('Find invalid characters:', ''.join(invalid_chars))\n",
    "\n",
    "    return invalid_chars\n",
    "\n",
    "def clean_poem_texts(poem_texts):\n",
    "    \"\"\"\n",
    "    Clean the poem texts by removing invalid characters and truncating to 24 characters.\n",
    "\n",
    "    :param poem_texts: pd.Series, the series of poem texts\n",
    "    :return: ndarray, cleaned poem texts with shape (n, 24)\n",
    "    \"\"\"\n",
    "    # Adjust the size of every poem item to 24\n",
    "    poems_truncated = poem_texts.map(lambda x: x[:24])\n",
    "\n",
    "    # Check the size less than 24\n",
    "    poems_small = poems_truncated[poems_truncated.str.len() < 24]\n",
    "    if not poems_small.empty:\n",
    "        print('Count of poems with size less than 24:', len(poems_small))\n",
    "        poems_truncated = poems_truncated[poems_truncated.str.len() == 24]\n",
    "\n",
    "    # Transform the pandas Series to numpy array\n",
    "    poem_numpy = np.array(\n",
    "        poems_truncated.map(list).to_list()\n",
    "    )\n",
    "    print('shape of numpy array', poem_numpy.shape)\n",
    "\n",
    "    # Check the signs in the wujue array\n",
    "    invalid_chars = check_punctuation(poem_numpy)\n",
    "\n",
    "    # Find the abnormal items\n",
    "    abnormal_items = poem_numpy[\n",
    "        np.isin(poem_numpy[:, [5, 11, 17, 23]], list(invalid_chars)).any(axis=1)\n",
    "    ]\n",
    "    abnormal_count = len(abnormal_items)\n",
    "    print('abnormal count:', abnormal_count)\n",
    "    if abnormal_count > 0:\n",
    "        print('abnormal item: ', ''.join(abnormal_items[0]))\n",
    "\n",
    "    # Remove the abnormal item\n",
    "    poems_removed_invalid = poem_numpy[\n",
    "        ~np.isin(poem_numpy[:, [5, 11, 17, 23]], list(invalid_chars)).any(axis=1)\n",
    "    ]\n",
    "\n",
    "    print('===== After removing the abnormal items =====')\n",
    "    print('shape of numpy array', poems_removed_invalid.shape)\n",
    "    check_punctuation(poems_removed_invalid)\n",
    "\n",
    "    # Convert back to pandas Series\n",
    "    return poems_removed_invalid"
   ],
   "id": "250d55c9d9594e57",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:11.776233Z",
     "start_time": "2025-08-28T13:00:09.603672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read all files and clean the dataset\n",
    "\n",
    "all_poems = []\n",
    "\n",
    "for file in poem_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    poem_texts = read_file_to_df(file)\n",
    "    if poem_texts.empty:\n",
    "        print(\"No valid poems found in this file.\\n\")\n",
    "        continue\n",
    "\n",
    "    result = clean_poem_texts(poem_texts)\n",
    "    all_poems.append(result)\n",
    "    print()\n",
    "\n",
    "# Concatenate all cleaned poems into a single array\n",
    "train_poems_numpy = np.concatenate(all_poems, axis=0)\n",
    "train_poems_numpy.shape"
   ],
   "id": "c85748c0068ea4ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data/Poetry/诗歌数据集/秦.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/先秦.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/隋.csv\n",
      "shape of numpy array (71, 24)\n",
      "Characters found at fixed positions: ，。？\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (71, 24)\n",
      "Characters found at fixed positions: ，。？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/辽.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/当代.csv\n",
      "shape of numpy array (1109, 24)\n",
      "Characters found at fixed positions: 华流。传！文不轻」鼠宵栏：？边山帽歌冕，闲大；对\n",
      "Find invalid characters: 华宵栏：边传山帽歌文对不轻冕」闲鼠大；流\n",
      "abnormal count: 13\n",
      "abnormal item:  琅玕经雨青，染绿山溪水。“萧萧我凭栏”，袅袅清歌\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1096, 24)\n",
      "Characters found at fixed positions: ，。！？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_1.csv\n",
      "shape of numpy array (3712, 24)\n",
      "Characters found at fixed positions: 者罗鹤，？人月相曲。丛兰》幕歌自金过\n",
      "Find invalid characters: 者罗鹤人月相曲丛幕兰金歌自》过\n",
      "abnormal count: 5\n",
      "abnormal item:  片舫连双桨，呕哑杂哩罗。惯听吴棹曲，羞杀《阆中歌\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3707, 24)\n",
      "Characters found at fixed positions: ，。？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_2.csv\n",
      "Count of poems with size less than 24: 3\n",
      "shape of numpy array (3829, 24)\n",
      "Characters found at fixed positions: ，。水？\n",
      "Find invalid characters: 水\n",
      "abnormal count: 1\n",
      "abnormal item:  操杵力不任，当垆心自鄙。花时掩关坐，焚香读《秋水\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3828, 24)\n",
      "Characters found at fixed positions: ，。？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_3.csv\n",
      "shape of numpy array (4249, 24)\n",
      "Characters found at fixed positions: ，？学筝。传！木；》\n",
      "Find invalid characters: 学传木》；筝\n",
      "abnormal count: 6\n",
      "abnormal item:  雨馀鸟语凉，斜阳竹深见。频来非看花，借读《高僧传\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (4243, 24)\n",
      "Characters found at fixed positions: ，。？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_3.csv\n",
      "shape of numpy array (1882, 24)\n",
      "Characters found at fixed positions: 天撞安销釭沾人3里话。乍！近轻梅外断蛾遥旧尔多敲？质尽日得饶通亲磨诉说1，生弱草真思；伸\n",
      "Find invalid characters: 天撞安销釭沾人3里话乍近轻梅外断蛾遥旧尔多敲质尽日得饶通亲磨诉说1生弱草真思；伸\n",
      "abnormal count: 16\n",
      "abnormal item:  昔子贵之时，曾居洛阳市。洛阳人情何？曰『薄乎云尔\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1866, 24)\n",
      "Characters found at fixed positions: ，。！？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_2.csv\n",
      "shape of numpy array (1473, 24)\n",
      "Characters found at fixed positions: ，？曲。有！；\n",
      "Find invalid characters: 曲有；\n",
      "abnormal count: 4\n",
      "abnormal item:  闻君向南投，曾过北投宿。试从鹿水头，一续「乌溪曲\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1469, 24)\n",
      "Characters found at fixed positions: ，。！？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/明_4.csv\n",
      "shape of numpy array (2184, 24)\n",
      "Characters found at fixed positions: 天，皇？。\n",
      "Find invalid characters: 皇天\n",
      "abnormal count: 1\n",
      "abnormal item:  去去去此间，不是留侬处。侬住三十三天天外天，玉皇\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2183, 24)\n",
      "Characters found at fixed positions: ，。？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/元.csv\n",
      "shape of numpy array (2624, 24)\n",
      "Characters found at fixed positions: 海。！痕两经南浮扇梅枝曲赋：？广苧相歌，篇星\n",
      "Find invalid characters: 赋：海广苧相痕歌两经南浮扇梅枝篇曲星\n",
      "abnormal count: 9\n",
      "abnormal item:  老竹空岩里，悬厓飞水前。欲识逍遥境，试读《逍遥篇\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2615, 24)\n",
      "Characters found at fixed positions: ，？。！\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/清_1.csv\n",
      "shape of numpy array (1711, 24)\n",
      "Characters found at fixed positions: 鹤，？城杜人。！水；潮箫\n",
      "Find invalid characters: 鹤城杜人水；箫潮\n",
      "abnormal count: 4\n",
      "abnormal item:  昔作秦淮客，朱楼赋《洞箫》。白头故人尽，重上石城\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1707, 24)\n",
      "Characters found at fixed positions: ，。！？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/南北朝.csv\n",
      "shape of numpy array (1, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (1, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_1.csv\n",
      "shape of numpy array (4397, 24)\n",
      "Characters found at fixed positions: ，桨。莲前；\n",
      "Find invalid characters: 莲桨前；\n",
      "abnormal count: 2\n",
      "abnormal item:  晚过鸳鸯浦，无心唱《采莲》。莫嗔兰桨急，为要趁前\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (4395, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_2.csv\n",
      "shape of numpy array (3470, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3470, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/唐.csv\n",
      "shape of numpy array (2711, 24)\n",
      "Characters found at fixed positions: 此沽，？。！\n",
      "Find invalid characters: 此沽\n",
      "abnormal count: 1\n",
      "abnormal item:  勒马问樵夫，前村酒有无。「杜康家在此，一任君来沽\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (2710, 24)\n",
      "Characters found at fixed positions: ，。！？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/近现代.csv\n",
      "shape of numpy array (398, 24)\n",
      "Characters found at fixed positions: ，？。！；\n",
      "Find invalid characters: ；\n",
      "abnormal count: 1\n",
      "abnormal item:  明月照秋霜，今朝还故乡；留得头颅在，雄心誓不降。\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (397, 24)\n",
      "Characters found at fixed positions: ，。！？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/宋_3.csv\n",
      "shape of numpy array (3005, 24)\n",
      "Characters found at fixed positions: 懒，？个曲。花灯\n",
      "Find invalid characters: 懒个曲花灯\n",
      "abnormal count: 3\n",
      "abnormal item:  明月入我池，皎皎铺纻缟。何日变成缁？《太玄》吾懒\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (3002, 24)\n",
      "Characters found at fixed positions: ，。？\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/汉.csv\n",
      "No valid poems found in this file.\n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/金.csv\n",
      "shape of numpy array (252, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "abnormal count: 0\n",
      "===== After removing the abnormal items =====\n",
      "shape of numpy array (252, 24)\n",
      "Characters found at fixed positions: ，。\n",
      "Find invalid characters: \n",
      "\n",
      "Processing file: data/Poetry/诗歌数据集/魏晋.csv\n",
      "No valid poems found in this file.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37012, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Dataset to token id sequences",
   "id": "44dc0adc2c6c2704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:13.603105Z",
     "start_time": "2025-08-28T13:00:11.832211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import layers\n",
    "\n",
    "tv = layers.TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    standardize=None,\n",
    "    split=None, # 直接喂二维数组\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=24\n",
    ")\n",
    "tv.adapt(train_poems_numpy)"
   ],
   "id": "c890cc6fbcb7117",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 21:00:12.055698: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-28 21:00:12.104845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756386012.129776    2647 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756386012.138510    2647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756386012.189269    2647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756386012.189288    2647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756386012.189292    2647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756386012.189294    2647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-28 21:00:12.197031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1756386013.449898    2647 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14126 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:14.067720Z",
     "start_time": "2025-08-28T13:00:13.652729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demo usage of tv\n",
    "\n",
    "# Print the vocabulary\n",
    "print('Vocabulary size:', tv.vocabulary_size())\n",
    "print('Vocabulary:', ''.join(tv.get_vocabulary()[:20]))\n",
    "\n",
    "# Encode\n",
    "encoded = tv(train_poems_numpy[0])\n",
    "\n",
    "# Decode\n",
    "vocab = tv.get_vocabulary()\n",
    "decoded =[vocab[i] for i in encoded]\n",
    "print('Encoded:', encoded.numpy())\n",
    "print('Decoded:', ''.join(decoded))"
   ],
   "id": "ef849f42f169ce1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6350\n",
      "Vocabulary: [UNK]，。不人山风一花无来何云有日月春水中\n",
      "Encoded: [1152 1152  948  466   65    2 1074  177  740   41  604    3 1103   64\n",
      "    6   16 1230    2  945 2176  183    7   23    3]\n",
      "Decoded: 脉脉广川流，驱马历长洲。鹊飞山月曙，蝉噪野风秋。\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:14.200465Z",
     "start_time": "2025-08-28T13:00:14.117157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode all the poems\n",
    "train_token_ids = tv(train_poems_numpy)\n",
    "print('shape of wujue_token_ids:', train_token_ids.shape)"
   ],
   "id": "8277b7ae292b1f71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of wujue_token_ids: (37012, 24)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Build the LSTM Decoder model",
   "id": "6145e1fc59ad943a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:14.666901Z",
     "start_time": "2025-08-28T13:00:14.650654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the train dataset\n",
    "train_sequences = train_token_ids[:, :-1]\n",
    "target_sequences = train_token_ids[:, 1:]\n",
    "\n",
    "train_sequences.shape, target_sequences.shape"
   ],
   "id": "43b8b72f51002c3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([37012, 23]), TensorShape([37012, 23]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:15.249235Z",
     "start_time": "2025-08-28T13:00:14.764263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build a simple LSTM Decoder model\n",
    "\n",
    "import keras\n",
    "from keras import models, layers\n",
    "\n",
    "def build_model(vocab_size):\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"inputs\")\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=100, name=\"embedding\")(inputs)\n",
    "    x = layers.LSTM(512, return_sequences=True, name=\"lstm\")(x)\n",
    "    x = layers.Dropout(0.1, name=\"dropout\")(x)\n",
    "    outputs = layers.Dense(vocab_size, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs, name=\"lstm_decoder\")\n",
    "\n",
    "model = build_model(tv.vocabulary_size())\n",
    "model.summary()"
   ],
   "id": "75954408bd3f32c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"lstm_decoder\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"lstm_decoder\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (\u001B[38;5;33mInputLayer\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m)      │       \u001B[38;5;34m635,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │     \u001B[38;5;34m1,255,424\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001B[38;5;33mDense\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6350\u001B[0m)     │     \u001B[38;5;34m3,257,550\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">635,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,255,424</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6350</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,257,550</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m5,147,974\u001B[0m (19.64 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,147,974</span> (19.64 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m5,147,974\u001B[0m (19.64 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,147,974</span> (19.64 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:16.764286Z",
     "start_time": "2025-08-28T13:00:15.344835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the sample generate function\n",
    "def generate(prompt, max_length=24, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate a poem based on the start prompt\n",
    "\n",
    "    Returns:\n",
    "        A generated poem as a string.\n",
    "    \"\"\"\n",
    "    prompt_inputs = list(prompt)\n",
    "    generated = tv(prompt_inputs)[:len(prompt)].numpy().tolist()\n",
    "    while len(generated) < max_length:\n",
    "        input_sequence = np.array(generated).reshape(1, -1)\n",
    "        predictions = model.predict(input_sequence, verbose=0)[0]\n",
    "        next_token_id = sample(predictions[-1], temperature)\n",
    "        generated.append(next_token_id)\n",
    "    return ''.join(tv.get_vocabulary()[token_id] for token_id in generated)\n",
    "\n",
    "def sample(predictions, temperature=1.0, eps1=1e-20, eps2=1e-9):\n",
    "    p = np.asarray(predictions, dtype=np.float64)\n",
    "\n",
    "    # The two key points: log(p + eps1) divide by (T + eps2)\n",
    "    logits = np.log(p + eps1) / (float(temperature) + eps2)\n",
    "\n",
    "    # Subtract the max logit to prevent overflow\n",
    "    logits -= np.max(logits)\n",
    "\n",
    "    q = np.exp(logits)\n",
    "    q /= q.sum()\n",
    "    return int(np.random.choice(len(q), p=q))\n",
    "\n",
    "\n",
    "generate(\"海外\", temperature=0)"
   ],
   "id": "2145bf31d5cc14b2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756386015.801194    2708 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'海外震震震帽震震噞噞姤帽价砻颓抢颓抢外败渫嚭自盲'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:00:16.813253Z",
     "start_time": "2025-08-28T13:00:16.806343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define callback to print the sample generative poem every 10 epochs\n",
    "class PoetryGenerateCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, epochs):\n",
    "        super().__init__()\n",
    "        self.generating_epochs = self._get_generating_epochs(epochs)\n",
    "        self.generated_poems = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch += 1\n",
    "        if epoch not in self.generating_epochs:\n",
    "            return\n",
    "        poems = self.generate_poems()\n",
    "        self.generated_poems[epoch] = {\n",
    "            'poems': poems,\n",
    "            'logs': logs\n",
    "        }\n",
    "\n",
    "        self.print_poems(poems)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_poems():\n",
    "        temperatures = [0, 0.5, 1.0, 1.5]\n",
    "        generated_texts = [\n",
    "            generate('海外', max_length=24, temperature=temp)\n",
    "            for temp in temperatures\n",
    "        ]\n",
    "        return [{ 'temperature': temperature, 'text': text } for temperature, text in zip(temperatures, generated_texts)]\n",
    "\n",
    "    @staticmethod\n",
    "    def print_poems(poems):\n",
    "        for item in poems:\n",
    "            print(f\"temperature {item['temperature']:.1f}: {item['text']}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_generating_epochs(epochs):\n",
    "        if epochs % 2 != 0:\n",
    "            print(\"Warning: epochs should be even number.\")\n",
    "\n",
    "        mid_epoch = epochs // 2\n",
    "        left_generating_epochs = [2**i for i in range(0, int(np.log2(mid_epoch)) + 1)]\n",
    "        if mid_epoch not in left_generating_epochs:\n",
    "            left_generating_epochs.append(mid_epoch)\n",
    "\n",
    "        right_generating_epochs = [1 + epochs - e for e in left_generating_epochs][::-1]\n",
    "        return left_generating_epochs + right_generating_epochs"
   ],
   "id": "c3bcded08b52ca33",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:11:33.157559Z",
     "start_time": "2025-08-28T13:00:16.943738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I have figured out the best epochs for training, thus no need to use the print callback here\n",
    "epochs = 50\n",
    "poetry_callback = PoetryGenerateCallback(epochs)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    train_sequences,\n",
    "    target_sequences,\n",
    "    batch_size=256,\n",
    "    epochs=epochs,\n",
    "    callbacks=[],\n",
    "    verbose=2\n",
    ")"
   ],
   "id": "9942b787f254ced1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "145/145 - 15s - 103ms/step - accuracy: 0.1031 - loss: 6.5479\n",
      "Epoch 2/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.1524 - loss: 5.8641\n",
      "Epoch 3/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.1907 - loss: 5.6873\n",
      "Epoch 4/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.1936 - loss: 5.6005\n",
      "Epoch 5/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.1962 - loss: 5.5137\n",
      "Epoch 6/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.1993 - loss: 5.4398\n",
      "Epoch 7/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2043 - loss: 5.3549\n",
      "Epoch 8/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2109 - loss: 5.2486\n",
      "Epoch 9/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2166 - loss: 5.1536\n",
      "Epoch 10/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2215 - loss: 5.0700\n",
      "Epoch 11/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2261 - loss: 5.0007\n",
      "Epoch 12/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2302 - loss: 4.9409\n",
      "Epoch 13/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2340 - loss: 4.8878\n",
      "Epoch 14/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2379 - loss: 4.8369\n",
      "Epoch 15/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2415 - loss: 4.7893\n",
      "Epoch 16/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2450 - loss: 4.7436\n",
      "Epoch 17/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2484 - loss: 4.6979\n",
      "Epoch 18/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2514 - loss: 4.6544\n",
      "Epoch 19/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2546 - loss: 4.6130\n",
      "Epoch 20/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2575 - loss: 4.5729\n",
      "Epoch 21/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2606 - loss: 4.5335\n",
      "Epoch 22/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2633 - loss: 4.4971\n",
      "Epoch 23/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2664 - loss: 4.4603\n",
      "Epoch 24/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2686 - loss: 4.4261\n",
      "Epoch 25/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2712 - loss: 4.3922\n",
      "Epoch 26/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2737 - loss: 4.3590\n",
      "Epoch 27/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2760 - loss: 4.3278\n",
      "Epoch 28/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2782 - loss: 4.2971\n",
      "Epoch 29/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2805 - loss: 4.2665\n",
      "Epoch 30/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2828 - loss: 4.2370\n",
      "Epoch 31/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2850 - loss: 4.2085\n",
      "Epoch 32/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2870 - loss: 4.1802\n",
      "Epoch 33/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2892 - loss: 4.1524\n",
      "Epoch 34/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2914 - loss: 4.1256\n",
      "Epoch 35/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2930 - loss: 4.1002\n",
      "Epoch 36/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.2952 - loss: 4.0738\n",
      "Epoch 37/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2975 - loss: 4.0479\n",
      "Epoch 38/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.2995 - loss: 4.0227\n",
      "Epoch 39/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.3017 - loss: 3.9985\n",
      "Epoch 40/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.3035 - loss: 3.9746\n",
      "Epoch 41/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.3060 - loss: 3.9511\n",
      "Epoch 42/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.3081 - loss: 3.9267\n",
      "Epoch 43/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.3098 - loss: 3.9040\n",
      "Epoch 44/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.3117 - loss: 3.8816\n",
      "Epoch 45/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.3135 - loss: 3.8595\n",
      "Epoch 46/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.3160 - loss: 3.8373\n",
      "Epoch 47/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.3179 - loss: 3.8148\n",
      "Epoch 48/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.3199 - loss: 3.7946\n",
      "Epoch 49/50\n",
      "145/145 - 14s - 93ms/step - accuracy: 0.3219 - loss: 3.7732\n",
      "Epoch 50/50\n",
      "145/145 - 13s - 93ms/step - accuracy: 0.3240 - loss: 3.7510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7efbde173710>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:11:33.311188Z",
     "start_time": "2025-08-28T13:11:33.305421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the generated poems for analysis\n",
    "# poetry_callback.generated_poems"
   ],
   "id": "f4fbfd0e7777f8d0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:17:03.419467Z",
     "start_time": "2025-08-28T13:17:03.176398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the vocabulary and model\n",
    "!mkdir -p models\n",
    "\n",
    "vocab = tv.get_vocabulary()\n",
    "with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "    for token in tv.get_vocabulary():\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "model.save(MODEL_PATH)"
   ],
   "id": "73e41d22c225ca47",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Using the model to generate poems",
   "id": "dada31118ce72d4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:17:06.978524Z",
     "start_time": "2025-08-28T13:17:06.834023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload the model\n",
    "\n",
    "with open(VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "tv = layers.TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    standardize=None,\n",
    "    split=None, # 直接喂二维数组\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=24\n",
    ")\n",
    "tv.set_vocabulary(vocab)\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH, compile=False)"
   ],
   "id": "76edf563676fd6c1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:17:12.385827Z",
     "start_time": "2025-08-28T13:17:08.835616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "poems = PoetryGenerateCallback.generate_poems()\n",
    "PoetryGenerateCallback.print_poems(poems)"
   ],
   "id": "d1d91eaf6923169e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature 0.0: 海外春风早，春风吹客衣。春风吹不尽，花落落花花。\n",
      "temperature 0.5: 海外春风早，梅花落日迟。春风吹不断，无奈老人知。\n",
      "temperature 1.0: 海外不高杂，桃花路路轻。林塘有种石，不为鼻林栖。\n",
      "temperature 1.5: 海外鸟涧谷，苍然叶石数。山真似因雷，置颈溪与去。\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
